# ML Anomaly Detection Pipeline (End-to-End)

![Build Status](https://img.shields.io/github/actions/workflow/status/parthamehta123/ml-anomaly-pipeline/ci.yml?branch=main)
![Terraform](https://img.shields.io/badge/Terraform-AWS%20EKS-blueviolet?logo=terraform)
![MLflow](https://img.shields.io/badge/MLflow-Latest%20Deployed-green?logo=mlflow)
![License](https://img.shields.io/badge/license-MIT-green)

This repository implements a **production-ready MLOps pipeline** for anomaly detection and time-series forecasting of infrastructure metrics (CPU, memory, disk I/O, pod restarts, etc.) using **Prometheus, Kubernetes, Terraform, AWS, MLflow, and ArgoCD GitOps**.

---

## ğŸ“Œ Features

- **Data Ingestion**
  - Prometheus metrics scraper
  - Synthetic metrics generator + Prometheus exporter for local testing
  - Metrics stored in S3

- **Preprocessing**
  - Sliding windows, z-score normalization
  - Seasonality adjustment for time-series

- **Model Training**
  - **Anomaly Detection**: Isolation Forest
  - **Forecasting**: ARIMA
  - **Hybrid Approach**: Forecast normal baseline + detect residual anomalies
  - MLflow used for experiment tracking

- **Model Serving**
  - FastAPI REST API
  - Packaged in Docker
  - Deployed on EKS with Helm
  - Horizontal Pod Autoscaling

- **Monitoring & Alerting**
  - Prometheus scrapes metrics
  - Grafana dashboards for visualization
  - Grafana alert rules (CPU > 85%) â†’ Slack / PagerDuty

- **Auto-Remediation**
  - Kubernetes Operator to restart pods
  - AWS Lambda to restart EC2 instances
  - Step Functions retraining pipeline triggered on persistent anomalies

- **MLOps**
  - Champion/Challenger model evaluation
  - MLflow experiment tracking (metrics, params, artifacts)
  - Retraining orchestrated with AWS Step Functions
  - **Auto-Rollback** on failed Terraform apply â†’ previous `mlflow_version` restored automatically

- **Infrastructure as Code**
  - Terraform provisions:
    - EKS cluster
    - RDS (Postgres backend for MLflow)
    - S3 (artifacts, metrics, models)
    - IAM roles
  - GitOps (ArgoCD App of Apps) manages all Helm deployments:
    - MLflow Tracking Server
    - Anomaly API
    - Prometheus + Grafana
    - Auto-Remediator

---

## ğŸ“‚ Repository Structure

```
ml-anomaly-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile
â”œâ”€â”€ terraform/                 # Terraform IaC
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ mlflow_rds.tf
â”‚   â”œâ”€â”€ mlflow_s3.tf
â”‚   â”œâ”€â”€ mlflow_helm.tf
â”œâ”€â”€ ml/                        # ML training + inference
â”‚   â”œâ”€â”€ generate_metrics.py
â”‚   â”œâ”€â”€ prometheus_exporter.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ inference.py
â”œâ”€â”€ api/                       # Serving API
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ monitoring/                # Monitoring + Alerting
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â”œâ”€â”€ grafana_dashboard.json
â”‚   â””â”€â”€ grafana_alerts.json
â”œâ”€â”€ lambda/                    # AWS Lambda functions
â”‚   â”œâ”€â”€ evaluate_model.py
â”‚   â””â”€â”€ trigger_stepfn.py
â”œâ”€â”€ stepfunctions/             # Step Functions definitions
â”‚   â””â”€â”€ anomaly_retrain.asl.json
â”œâ”€â”€ gitops/                    # ArgoCD GitOps
â”‚   â”œâ”€â”€ root-app.yaml
â”‚   â”œâ”€â”€ mlflow-app.yaml
â”‚   â”œâ”€â”€ anomaly-api-app.yaml
â”‚   â”œâ”€â”€ prometheus-app.yaml
â”‚   â”œâ”€â”€ grafana-app.yaml
â”‚   â””â”€â”€ operator-app.yaml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.png
â”‚   â”œâ”€â”€ demo.gif
â”‚   â”œâ”€â”€ mlflow_pipeline_flowchart.png
â”‚   â”œâ”€â”€ mlflow_pipeline_flowchart.md
â”‚   â””â”€â”€ mlflow_pipeline_flowchart.mmd
```

---

## ğŸš€ Quickstart

### 1. Generate synthetic dataset
```bash
python ml/generate_metrics.py
python ml/preprocess.py
python ml/train.py
```

### 2. Run Prometheus exporter locally
```bash
python ml/prometheus_exporter.py
# Metrics at http://localhost:8000/metrics
```

### 3. Train & serve model locally
```bash
uvicorn api.app:app --reload --port 8080
curl -X POST http://localhost:8080/predict -H "Content-Type: application/json" -d '{"cpu_zscore": 2.1, "memory_zscore": 1.5}'
```

### 4. Deploy Infra with Terraform
```bash
cd terraform
terraform init
terraform apply
```

### 5. Deploy Apps with ArgoCD
```bash
kubectl apply -f gitops/root-app.yaml -n argocd
```

---

## ğŸ”„ CI/CD

- GitHub Actions workflow:
  - Mirrors MLflow images into ECR.
  - Posts **Terraform upgrade instructions** to Slack/Teams.
  - Auto-bumps `mlflow_version` in `terraform.tfvars` via PR when new versions are mirrored.
  - Runs Terraform plan on PR, apply on merge.
  - On **failure** â†’ auto-rolls back to last good version and posts rollback notification.
- Drift correction ensures infra + MLflow always matches Git.

---

## ğŸ“Š Monitoring

- **Prometheus**: scrapes metrics every 5s  
- **Grafana Dashboard**: `monitoring/grafana_dashboard.json`  
- **Grafana Alerts**: `monitoring/grafana_alerts.json`  
- Example: CPU > 85% for 1 min â†’ Slack alert

---

## âœ… End-to-End Flow

1. Prometheus collects metrics  
2. Grafana visualizes + alerts  
3. Alert triggers remediation (K8s Operator / Lambda)  
4. Step Functions retraining pipeline launches  
5. Champion vs Challenger evaluated â†’ MLflow tracks experiments  
6. Best model deployed via FastAPI on EKS  

---

## ğŸ–¼ï¸ Architecture Diagram

![Architecture](docs/architecture.png)

This diagram shows the full pipeline:
- **Telemetry** â†’ Prometheus exporter, CloudWatch
- **ML Layer** â†’ Preprocessing, Isolation Forest, ARIMA, MLflow
- **Serving** â†’ FastAPI API on EKS (Helm + Docker)
- **Monitoring** â†’ Prometheus, Grafana dashboards & alerts
- **Chaos Testing** â†’ Chaos Mesh, AWS FIS, Locust
- **Auto-Remediation** â†’ K8s Operator, AWS Lambda, Step Functions

---

## ğŸ”„ CI/CD + Rollback Flow (Mermaid)

```mermaid
flowchart LR
    mirror[ğŸ”„ Mirror MLflow<br/>(DockerHub â†’ ECR)]
    notify[ğŸ“¢ Notify Slack/Teams<br/>+ Terraform instructions]
    bump[ğŸ¤– Bump Workflow<br/>(Update terraform.tfvars,<br/>Open PR)]
    plan[ğŸ“ Terraform Plan<br/>(Comment on PR)]
    apply[ğŸš€ Terraform Apply<br/>(on merge to main)]
    success((âœ… Success<br/>Slack + Teams Notify))
    fail((âŒ Failure<br/>Slack + Teams Alert))
    rollback[ğŸ”„ Auto-Rollback PR<br/>+ Auto-Merge]
    rollback_notify((ğŸ“¢ Rollback Success<br/>Slack + Teams Notify))

    mirror --> notify
    mirror --> bump
    bump --> plan
    plan --> apply
    apply -->|success| success
    apply -->|failure| fail
    fail --> rollback
    rollback --> rollback_notify
```

---

## ğŸ¥ Demo (Chaos Test + Grafana Alert)

![Chaos Demo](docs/demo.gif)

*This demo shows a chaos experiment (pod kill) triggering anomalies â†’ Grafana alert firing â†’ auto-remediation + rollback kicking in.*
